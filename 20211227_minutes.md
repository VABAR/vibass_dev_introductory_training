
# Goal of the training

- Reformulate as follows:

Previous version:
> At the end of the course, participants will be able to start using Bayesian
statistics in their field, having acquired the foundations needed to learn and
apply more specific methods and models.

Proposed version:
> At the end of the course, participants will be able to start applying Bayesian
methods to simple models in their field, having acquired an understanding of
Bayesian concepts and becoming aware of the possibilities for more specific and
more complex methods for in-depth analysis.

Here we reduce the expectations by specifying __simple models__ and are more 
explicit about creating awareness of the possibilities.


# Operational goals 3 and 4

> Summarise inferences and predictions using random samples.

> Obtain samples of functions of parameters using random samples.

Merge into:
> Summarise inferences and predictions of quantities of interest and derived
functions using random samples from posterior probability distributions.



# Operational goals 5 and 6

> Interpret the information contained in a probability distribution by visual
inspection of its density or mass function.

> Assess the compatibility or discrepancy between probability distributions by
visual inspection of their densities or mass functions.

Merge into:
> Interpret the information contained in a posterior probability distribution by
visual inspection of its shape in a plot of the density or mass function.

Essentially removing the objective of comparing probability distributions.


# Operational goal 7

Previous version:
> Synthesise the main characteristics of the three basic methods for Bayesian
computation: analytical conjugate derivations, Importance Sampling and
Metropolis Hastings.


> Synthesise the main characteristics of three core approaches for Bayesian
computation: analytical conjugate derivations, Importance Sampling and Markov
Chain Monte Carlo.

Remove _the_ from _the three_, since there are other methods (like
numerical approximation) that we do not cover.

Replace _basic_ by _core_ to avoid judgement.

Replace _MH_ by the more general _MCMC_.


# Operational goals 8, 9 and 10

Previous versions:
> Adapt basic IS and MH algorithms for fitting (generalised) linear regression
models by sequentially sampling from each of them.

> Distinguish the proposal distribution in Importance Sampling from the prior
distribution, and assess its performance using the effective sample size.

> Fit hierarchical models with Bayesian software like INLA or BayesX.

Proposal:

> Apply basic Importance Sampling and MCMC methods via available software for
fitting regression models

We remove the accent on the programming skills needed to _adapt_ IS and MH code.
While some participants might appreciate it, is not the case of all, and it is
not strictly a requirement for getting started with Bayesian inference.

We will still provide the code for IS and MH, and give an overview of how it
works for interested participants. However, the main goal is to apply __some__
method (using the provided functions, programming methods, or using some package)
in order to obtain posterior distributions.

In order words, being able to apply some inference method is necessary for the
general objective of the course.


# Additions

We considered necessary to include the following additional operational goals
to the training:

- Assess the convergence quality of the Markov Chains by visual inspection 
their traces and by convergence indices like $\hat R$.

- Summarise the landscape of available software packages, and their
characteristics.

  Specifically, whether they are flexible probabilistic programming languages
  (Bugs, Stan, Nimble, ...) or interfaces to a closed set of families of 
  statistical models, and their relative strengths and weaknesses.

- Identify in which situations Bayesian analysis is particularly advantageous
with respect to frequentist alternatives.

  Specifically, the direct probabilistic interpretation of results, the treatment
  of the uncertainty in the model parameters (e.g. proper propagation, rather
  than marginalisation using MLEs), not relying on asymptotic theory,
  the probabilistic approach to hypothesis testing.

